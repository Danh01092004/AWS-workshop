[
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "How AWS Protects Customers from DDoS Events Overview Since late August 2023, AWS detected and protected against a new kind of DDoS event: an HTTP/2 request flood, specifically called an HTTP/2 rapid reset attack, targeting Amazon CloudFront. These floods suddenly sent huge volumes of HTTP/2 connection requests followed rapidly by resets, overwhelming servers by forcing them to log and parse requests before cancellation. :contentReference[oaicite:0]{index=0}\nKey Event Details The spike occurred August 28‚Äì29, 2023, peaking at over 155 million requests per second via CloudFront. :contentReference[oaicite:1]{index=1} AWS observed and mitigated multiple of these HTTP/2 rapid reset events in that period, then continued seeing them into September. :contentReference[oaicite:2]{index=2} Customers with DDoS-resilient architectures ‚Äî using services like CloudFront, AWS Shield ‚Äî maintained availability during those attacks. :contentReference[oaicite:3]{index=3} Understanding HTTP/2 Rapid Reset Attacks HTTP/2 allows multiple logical streams over one session, different from HTTP/1.x. :contentReference[oaicite:4]{index=4} Attackers issue many requests, then quickly reset each stream. Servers still incur overhead: parsing, logging, etc., even if response isn\u0026rsquo;t delivered. :contentReference[oaicite:5]{index=5} These are a subclass of HTTP request floods; important to detect and absorb malicious traffic before it reaches origin systems. :contentReference[oaicite:6]{index=6} AWS‚Äôs Defensive Measures and Best Practices AWS provides built-in DDoS protection across its infrastructure. :contentReference[oaicite:7]{index=7} Key services to use for resilience: Amazon CloudFront, AWS Shield (including Shield Advanced), AWS WAF, Amazon Route 53, Route 53 Application Recovery Controller. These help when serving traffic from global edge locations, preventing unnecessary requests from reaching origin servers. :contentReference[oaicite:8]{index=8} Use caching at edge (e.g. CloudFront), careful filtering (WAF), and global routing/resiliency (Route 53) to absorb or block malicious traffic. :contentReference[oaicite:9]{index=9} Threat Intelligence, Monitoring, and Collaboration AWS continuously monitors for unusual traffic patterns and works proactively to detect threats. :contentReference[oaicite:10]{index=10} Engineers combine global threat intelligence, telemetry, and automated mitigation to stay ahead of new DDoS methods. :contentReference[oaicite:11]{index=11} AWS collaborates with external entities (CERTs, ISPs, registrars, other cloud providers) to trace, mitigate, and sometimes dismantle sources of large DDoS attacks. :contentReference[oaicite:12]{index=12} Summary AWS responded swiftly to a novel HTTP/2 rapid reset DDoS attack in late August/early September 2023. Thanks to built-in protections + resilience-focused architectures using services like CloudFront, Shield, WAF, and Route 53, customers were able to maintain availability. Ongoing monitoring, threat intelligence, and infrastructure-level mitigations are central to AWS‚Äôs approach to protecting from evolving DDoS threats.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúAWS AI/ML \u0026amp; Generative AI Workshop‚Äù Event Objectives Introduce the AWS AI/ML ecosystem Provide an overview of core services such as Amazon SageMaker and Amazon Bedrock Demonstrate the end-to-end ML lifecycle Present key Generative AI concepts, prompt engineering, and RAG Support hands-on learning through demos and discussions with experts Workshop Agenda \u0026amp; Key Highlights 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; Introduction Activities Participant registration \u0026amp; networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam: enterprise adoption, talent development, and GenAI trends Highlights:\nProvided a big-picture understanding of the AI/ML growth in Vietnam Emphasized AWS‚Äôs role in supporting organizations adopting AI/ML safely and effectively 9:00 ‚Äì 10:30 AM | AWS AI/ML Services Overview 1. Amazon SageMaker ‚Äì End-to-End ML Platform Topics covered:\nData preparation \u0026amp; labeling with SageMaker Ground Truth Model training and hyperparameter tuning Model deployment through SageMaker Endpoints Integrated MLOps: pipelines, model registry, CI/CD workflows 2. Live Demo: SageMaker Studio Navigating SageMaker Studio Hands-on flow from data ‚Üí training ‚Üí inference Demonstration of auto-scaling endpoints and model versioning Key takeaways:\nSageMaker unifies all ML lifecycle components in a single platform Reduces operational overhead and accelerates production deployment Integrates seamlessly with AWS services such as S3, Lambda, and API Gateway 10:30 ‚Äì 10:45 AM | Coffee Break A short break for refreshments and informal discussion with experts.\n10:45 AM ‚Äì 12:00 PM | Generative AI with Amazon Bedrock 1. Foundation Models (FMs) Comparison and selection guidance:\nClaude ‚Äì strong reasoning, safety-first, enterprise-friendly Llama ‚Äì open-source, highly customizable Titan ‚Äì optimized for AWS ecosystem and enterprise use cases 2. Prompt Engineering Prompting techniques, Chain-of-Thought, and few-shot examples Strategies to increase accuracy and consistency Demo prompts across multiple models in Bedrock 3. Retrieval-Augmented Generation (RAG) RAG architecture and workflow explanation Creating Knowledge Bases in Bedrock How GenAI retrieves private enterprise data for accurate answers Best practices: chunking, embedding selection, vector search 4. Bedrock Agents Building multi-step workflows with tool integrations Use cases: customer support bots, enterprise assistants, workflow automation 5. Guardrails Configuring safety rules, blocked content, and filters Tailoring guardrails to enterprise needs Ensuring model outputs meet security and compliance requirements Key Takeaways System Design Mindset Choose the right foundation model for the right use case Use guardrails and controlled prompting for enterprise safety MLOps/GenAIOps are essential for reliability and governance Technical Knowledge SageMaker supports full ML pipelines end-to-end Bedrock enables building GenAI applications without training large models RAG is the key technique for enterprise knowledge integration Prompt Engineering directly affects output quality Application Strategy Start with small pilot projects using Bedrock Build a RAG-based knowledge assistant for internal documents Integrate SageMaker into existing workflows for ML automation Experiment with Bedrock Agents to optimize business processes Event Experience Learning from Experts Speakers provided valuable insights into AI/ML, both technical and strategic Real-world case studies made the concepts easier to apply Hands-on Exposure The SageMaker Studio demo clarified how end-to-end ML pipelines operate Bedrock demonstrations showed how quickly GenAI apps can be built Networking \u0026amp; Discussion Networking helped exchange best practices among participants Discussions covered data challenges, cost optimization, and model safety Lessons Learned AI/ML adoption requires a clear strategy, not trend following RAG and Guardrails are critical for safe and accurate GenAI solutions SageMaker and Bedrock significantly reduce time-to-market for AI initiatives Event Photos "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Hu·ª≥nh ƒê·ª©c Anh\nPhone Number: 0768010904\nEmail: anhhdse183114@fpt.edu.vn\nUniversity: FPTU\nMajor: Information Assurance\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/26/2025 08/26/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + Overview AWS 09/08/2025 09/08/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Create budget by template - Practice: + Set up with Virtual MFA device + Create an AWS account + Create admin group and admin user + Account authentication support\n+ Install \u0026amp; configure AWS CLI + Create Budget by template , Create cost budget Tutorial , Usage Budget in AWS , A reservation instance , A savings plans budget 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ https://000001.awsstudygroup.com/ https://000007.awsstudygroup.com/ https://000009.awsstudygroup.com/ 5 - Learn AWS Virtual Private Cloud + VPC Security and Multi-VPC features + VPC - DirecConnect - LoadBalancer - ExtraResources - Practice: + Start with Amazon VPC and AWS VPN Site-to-Site + Subnets, Route table,Internet Gateway, Nat Gateway, Security Group, Network ACLs + Create VPC , VPC Resource Map, Create Subnet, Create an Internet Gateway, Create Route Table for Outbound Internet Routing via Internet Gateway, Create security groups + Create EC2 Instances in Subnets, Test connection, Create NAT Gateway, EC2 Instance Connect Endpoint 09/11/2025 09/16/2025 https://000009.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 09/12/2025 09/12/2025 https://000009.awsstudygroup.com/ Week 1 Achievements: Became acquainted with FCJ members. Read and took notes on the internship unit‚Äôs rules and regulations. Learned about AWS and its main service categories: Compute Storage Networking Database AWS Overview Created an AWS Free Tier account. Explored the AWS Management Console and AWS CLI. Created a Budget template and studied Cost Budget, Usage Budget, Reservation Instance, and Savings Plan Budget. Set up MFA (Virtual MFA device), created an Admin group and Admin user, and supported account authentication. Learned about AWS VPC (VPC Security, Multi-VPC features, DirectConnect, Load Balancer, Extra resources). Hands-on practice: Created VPC, Subnet, Internet Gateway, Route Table, Security Groups, NAT Gateway. Launched and connected EC2 Instances within Subnets. Practiced launching an EC2 instance, connecting via SSH, and attaching an EBS volume. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What* did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: AWS VPC, EC2, Networking Fundamentals \u0026amp; Route 53 DNS Setup\nWeek 3: VPC, EC2, Storage Management \u0026amp; Hands-on Networking Practice\nWeek 4: Amazon S3 Static Website Hosting \u0026amp; Amazon RDS Database Setup\nWeek 5: WordPress Deployment, CloudFront CDN \u0026amp; AWS Managed AD Integration\nWeek 6: Serverless Architecture: Lambda, API Gateway, DynamoDB \u0026amp; CloudFormation\nWeek 7: Container Technology: Docker, ECR, ECS Fargate \u0026amp; CI/CD Pipeline\nWeek 8: Advanced Networking: VPC Peering, Transit Gateway, Direct Connect \u0026amp; Hybrid Cloud\nWeek 9: CI/CD Automation: CodePipeline, CodeBuild, CodeDeploy \u0026amp; Safe Deployments\nWeek 10: VM Migration with VMWare, S3 Management, Storage Gateway \u0026amp; CloudFront\nWeek 11: AWS Security: IAM, Cognito, Security Hub, KMS \u0026amp; Resource Tagging\nWeek 12: Data Services: S3, Glue, Athena, DynamoDB, QuickSight \u0026amp; Data Analytics\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AWS First Cloud AI Journey ‚Äì Project Plan Hello World ‚Äì FPT University ‚Äì EveryoneCook\nDate: 30/11/2025\nüì• Download Full Proposal (DOCX)\nTABLE OF CONTENTS 1. BACKGROUND AND MOTIVATION\n1.1 executive summary\n1.2 PROJECT SUCCESS CRITERIA\n1.3 Assumptions\n2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 Technical Architecture Diagram\n2.2 Technical Plan\n2.3 Project Plan\n2.4 Security Considerations\n3. ACTIVITIES AND DELIVERABLES\n3.1 Activities and deliverables\n3.2 OUT OF SCOPE\n3.3 PATH TO PRODUCTION\n4. EXPECTED AWS COST BREAKDOWN BY SERVICES\n5. TEAM\n6. RESOURCES \u0026amp; COST ESTIMATES\n7. ACCEPTANCE\n1.BACKGROUND and motivation 1.1 Executive summary Customer background\nThe customer is a startup focused on building a modern social network platform where users can share cooking recipes, upload food photos, exchange culinary experiences, and explore meals recommended by AI. The organization aims to deliver a highly interactive platform capable of serving a large and growing user base.\nBusiness and technical objectives \u0026ndash; drivers for moving to the AWS cloud\nEnable rapid development and deployment using AWS managed services Ensure high scalability as the user base and media storage grow Provide a reliable, low-latency environment for AI computation and content delivery Reduce upfront infrastructure cost and move toward a pay-as-you-go model Improve data security, backup, and compliance through AWS-native capabilities Use cases\nUsers upload recipes, photos, and cooking videos to the platform System recommends dishes using AI based on available ingredients provided by the user Users interact socially through liking, commenting, sharing, and following AI processes text and images to generate recipe suggestions Admins manage content, monitor platform activity, and track performance analytics To meet the customer\u0026rsquo;s objectives of building a scalable social cooking platform with AI-powered recipe recommendations, the partner will deliver a full end-to-end cloud implementation on AWS. The services provided include:\nCloud Architecture Design: Define a secure, highly scalable, serverless architecture using AWS best practices (Route 53, API Gateway, Lambda, DynamoDB, S3, CloudFront, Cognito) AI Integration: Implement AWS Bedrock (Claude 3.5 Sonnet) for intelligent recipe suggestions, image analysis, and natural language processing features Infrastructure Deployment: Build and deploy all backend, frontend, authentication, and data layers using Infrastructure as Code (IaC) with fully automated CI/CD pipelines Security \u0026amp; Compliance: Configure IAM roles, encryption (KMS), WAF, logging, monitoring, and compliance guardrails to ensure platform security Observability Setup: Enable CloudWatch dashboards, alarms, X-Ray tracing, and log centralization for real-time monitoring and performance insights DevOps \u0026amp; Automation: Implement automated build/deploy workflows via GitLab + Amplify, operational pipelines, and auto-scaling configurations Performance Optimization: Configure CDN caching, DynamoDB capacity scaling, search indexing, and asynchronous SQS-based background processing Knowledge Transfer \u0026amp; Documentation: Provide technical documentation, best practices, architectural guides, and handover training to the customer\u0026rsquo;s engineering team 1.2 Project Success Criteria Project Success Criteria\nSystem availability ‚â• 99.9% uptime across all production services (API Gateway, Lambda, DynamoDB, CloudFront).\nPage load time \u0026lt; 2.5 seconds for the main user interface delivered through CloudFront and Amplify.\nAPI response time \u0026lt; 300 ms for 90% of all user-facing API requests under normal traffic conditions.\nAI processing latency \u0026lt; 5 seconds for recipe suggestions generated by AWS Bedrock.\nUser authentication success rate ‚â• 98% with Cognito handling registration, login, and email verification.\nZero critical security vulnerabilities after security review and WAF rules deployment.\nData durability of 99.999999999% (11 nines) ensured through S3 object storage and DynamoDB.\nScalability to support 10,000+ concurrent users without degradation in performance due to serverless infrastructure.\nOperational cost control within target budget: monthly AWS usage not exceeding $200 for production.\nImage upload \u0026amp; processing success rate ‚â• 99%, supported by S3, Lambda Workers, and SQS.\nSearch performance under 1 second (if OpenSearch is enabled) for recipe/content search queries.\nMonitoring coverage of 100% critical services using CloudWatch dashboards, alarms, and X-Ray tracing.\nCI/CD deployment time \u0026lt; 5 minutes via GitHub ‚Üí Amplify and IaC automation.\nZero data loss events, ensured by DynamoDB PITR and S3 versioning.\n1.3 Assumptions The customer will provide full access to their domain registrar (Hostinger) to configure DNS delegation to Route 53.\nThe customer will provide valid AWS account access with Administrator privileges for deployment and configuration.\nAll required AWS services (Amplify, API Gateway, Lambda, DynamoDB, CloudFront, Cognito, Bedrock, SES) are available and supported in the chosen region.\nSES will be successfully moved out of the sandbox and approved for production email sending.\nThird-party integrations (GitHub for CI/CD, external email clients, image upload sources) will remain available and stable.\nThe development team will maintain source code quality and follow the architectural guidelines provided by the partner.\nThe customer will provide timely feedback and approvals during design, testing, and deployment phases.\nDependencies\nReliable internet connectivity is required for all users accessing the web application and APIs.\nThe system depends on AWS Bedrock (Claude 3.5 Sonnet) for AI recipe generation and may experience performance fluctuations if the model becomes rate-limited.\nImage upload and processing workflows depend on S3, Lambda, and SQS processing reliability.\nIf OpenSearch is enabled, search features rely on the availability of the OpenSearch domain.\nGitHub Actions and Amplify depend on GitHub service availability.\nConstraints\nThe project will be fully deployed in a single AWS region, which may impact latency for users outside the region.\nThe solution is designed using serverless patterns; custom EC2-based workloads are outside the project scope.\nSES domain reputation may affect email deliverability during initial weeks.\nOpenSearch is deployed as a single-node cluster for cost efficiency, which means no high availability for search indexing.\nThe system must stay within the customer\u0026rsquo;s cost target (\u0026lt; $200/month), limiting the use of large compute resources.\nRisks\nSES production approval may be delayed, impacting user onboarding emails and notifications.\nIf traffic grows unexpectedly, DynamoDB provisioned capacity may throttle without timely scaling adjustments.\nAI model cost or latency changes by AWS may impact application performance or cost control.\nMisconfigured CloudFront caching could lead to higher latency or increased data transfer cost.\nAny incorrect IAM configuration could lead to security risks or service disruption.\nCustomer team turnover or lack of DevOps skills may slow down future maintenance or deployments.\n2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram The proposed solution architecture for the AI-powered cooking social network platform is designed using a fully serverless and scalable AWS cloud-native stack. The architecture ensures high availability,security, and seamless integration between the web frontend, backend APIs, authentication, data storage, and AI recommendation services. Below is a description of the key components and how data flows across the system: 1. Network \u0026amp; Edge Layer\nAmazon Route 53 (6\u0026ndash;7)\nProvides DNS routing for the custom domain used by the platform. Incoming HTTPS requests from users are resolved and forwarded to CloudFront. Amazon CloudFront (9)\nActs as a global CDN distributing frontend content with low latency while caching static files. AWS WAF (8)\nProtects the application from common web exploits such as SQL injection, XSS, and bot attacks. 2. Frontend Hosting \u0026amp; Deployment\nAWS Amplify Hosting (4)\nHosts and deploys the Next.js frontend application. Integrated with GitLab CI/CD (3) for automated deployments from the development workflow. 3. Application Layer\nAmazon Cognito (10)\nHandles user authentication and authorization, supporting email/password and social logins. Amazon API Gateway (11)\nServes as the main entry point for backend APIs, exposing REST endpoints used by the frontend. AWS Lambda (12, 15)\nContains the backend business logic, including:\nuser management\npost and recipe operations\ningredient analysis\nconnecting to Bedrock for AI recommendations\nThis serverless architecture ensures automatic scaling and pay-per-use cost efficiency.\n4. AI Recommendation Layer\nAmazon Bedrock (16\u0026ndash;17)\nProvides generative AI capabilities to suggest recipes based on user-provided ingredients.\nLambda invokes Bedrock models (e.g., Claude, Titan) to:\nanalyze ingredient lists\ngenerate recipe suggestions\nclassify food categories\noptimize cooking steps.\n5. Data Storage Layer\nAmazon DynamoDB (13)\nStores structured application data such as:\nuser profiles\nposts/recipes\nlikes \u0026amp; comments\ningredient metadata.\nAmazon S3 (14)\nStores unstructured data:\nrecipe images\nuser-uploaded food photos\nstatic content.\nAn S3 bucket is integrated with CloudFront via OAI for secure access.\n6. Observability \u0026amp; Security Layer\nAmazon CloudWatch (Logs \u0026amp; Metrics)\nMonitors Lambda performance, API Gateway access logs, and system metrics.\nAWS X-Ray\nPerforms distributed tracing for API calls and debugging.\nIAM\nDefines permission boundaries between API, Lambda functions, Bedrock, DynamoDB, and S3.\nAmazon SES\nSends verification emails, notifications, and password recovery messages.\nAmazon SNS\nHandles system-level alerts and asynchronous messaging.\n7. Deployment \u0026amp; Infrastructure Management\nAWS CDK (1\u0026ndash;2)\nUsed by developers to define and provision the entire infrastructure via CloudFormation templates.\nEnsures consistent, reproducible, version-controlled deployments. 2.2 Technical Plan Partner will develop Infrastructure-as-Code (IaC) automation using AWS CDK (Cloud Development Kit) with TypeScript/Python to provision the entire cloud environment. This approach ensures quick, consistent, and repeatable deployments across multiple AWS accounts and environments (dev, staging, production). All resources such as API Gateway, Lambda functions, DynamoDB tables, S3 buckets, Cognito user pools, Bedrock integration policies, and CloudFront distributions will be fully automated via IaC.\nApplication build and deployment processes for the frontend (Next.js) will be automated using AWS Amplify Hosting, integrated with GitLab pipelines. Backend components will be deployed through CDK pipelines to ensure controlled, versioned, and repeatable releases.\nSome additional configuration such as custom domain setup, Route 53 DNS changes, SSL/TLS certificate issuance, and IAM permission approvals may require customer review and approval. These changes will follow the customer's existing change management process, including scheduled maintenance windows and documented approvals from the security/compliance teams.\nAll critical paths, including authentication flows, AI recipe suggestion APIs, data persistence logic, and image upload workflows, will undergo extensive test coverage. Automated tests (unit, integration, and API-level) will be executed in CI/CD pipelines, and manual validation will be performed in the staging environment before production deployment.\n2.3 Project Plan [Partner] will adopt the Agile Scrum framework across eight 2-week sprints. Stakeholders from the team are required to participate in Sprint Reviews and Sprint Retrospectives to ensure alignment and continuous improvement.\nThe proposed team responsibilities are as follows:\nProduct Owner: Define user stories, prioritize backlog, and ensure the product meets user needs.\nScrum Master**:** Facilitate Scrum ceremonies, remove blockers, and maintain team productivity.\nDevelopment Team: Implement features, conduct unit testing, and collaborate on integration.\nAI/ML Specialist: Develop and fine-tune the AI recommendation engine that suggests recipes based on user-provided ingredients.\nUI/UX Designer: Design intuitive interfaces and ensure a smooth user experience on both web and mobile platforms.\nQA/Testers**:** Validate feature functionality, conduct regression testing, and ensure system reliability.\nCommunication cadences will be established as follows:\nDaily Stand-ups**:** 15-minute meetings for progress updates and immediate blockers.\nSprint Planning**:** At the start of each sprint to prioritize tasks.\nSprint Review: At the end of each sprint to showcase completed features to stakeholders.\nSprint Retrospective**:** Following each sprint review to identify improvements for the next sprint.\nKnowledge transfer sessions will be conducted by the senior developers and AI specialists to ensure team members understand system architecture, AI integration, and deployment procedures.\n2.4 Security Considerations Partner will implement security best practices across the following five categories to ensure the confidentiality, integrity, and availability of the platform:\nAccess Enable Multi-Factor Authentication (MFA) for all user and administrative accounts.\nImplement role-based access control (RBAC) to limit permissions based on user roles (e.g., admin, moderator, contributor).\nEnforce strong password policies and periodic password rotation.\nInfrastructure Deploy the application on secure, managed cloud services (e.g., AWS) following AWS security best practices.\nUse Virtual Private Cloud (VPC), network segmentation, and security groups to isolate resources.\nRegularly patch operating systems and containerized services to mitigate vulnerabilities.\nData Encrypt all data at rest using AWS KMS-managed keys and data in transit using TLS/HTTPS.\nImplement data classification to protect sensitive user information (e.g., email, profile data, dietary preferences).\nApply secure data storage and backup procedures to ensure availability and integrity.\nDetection Enable AWS CloudTrail and AWS Config to monitor API activity and resource configurations.\nDeploy logging and alerting mechanisms to detect unusual or suspicious activities in real time.\nConduct periodic vulnerability scanning and penetration testing on the platform.\nIncident Management Establish a formal incident response plan including detection, containment, remediation, and communication.\nMaintain audit trails and logs to support forensic investigation if a security event occurs.\nEnsure [Customer] shares regulatory control validations to help [Partner] meet all compliance requirements (e.g., GDPR, local privacy regulations).\nBy adhering to these measures, Partner ensures that the social cooking platform remains secure, compliant, and resilient against potential threats.\n3. Activities AND Deliverables 3.1 Activities and deliverables [Provide project milestones with timeline and respective deliverables, corresponding to the items and activities described in the Scope of Work / Technical Project Plan section. Indicate plans on how to govern the project/ change management; communication plans; transition plans]\nProject Phase Timeline Activities Deliverables/Milestones Total man-day Infrastructure Setup Week 1-2 - Learn all aws service\n- Practice Lab¬†- Worklog¬†2 week Project Foundation \u0026amp; Infrastructure Setup Week 3 -¬†Initialize monorepo structure\n- setup development environment¬†- Initialize Git repository and CI/CD\n- Initialize CDK project structure\n-Create environment configuration system\n-Setup CDK deployment scripts\n-¬†Setup Git repository with CI/CD pipelines and branch protection\n-¬†Configure local testing scripts and Git hooks for code quality\n- Set up AWS CDK project structure with proper organization for infrastructure as code\n-¬†Implement centralized configuration management for dev, staging, and prod environments\n1 week -DNS Infrastructure (Route 53 Hosted Zone) - DNS Stack\n- Side Quest: CloudFront y√™u c·∫ßu ACM certificate ·ªü us-east-1, nh∆∞ng stack ch√≠nh deploy ·ªü ap-southeast-1\nWeek 4 ,5 - Create a Public Hosted Zone\n- Configure name server delegation\n- Architecture Design\n- Create DNS Stack in CDK project\n- Connect DNS , User Route 53 Alias targeting for AWS - managed¬†- Request ACM certificates, Configure DNS validation in Route 53\n- Configure MX records for email , add SPF,DKIM,DMARC for email authentication\n- Create DNS structure , Validate DNS\n-npm run cdk deploy EveryoneCook-dev-Certificate¬†-¬†Domain \u0026amp; Hosted Zone Setup\n- Deploy DNS Stack\n- Route 53 Hosted Zone Status\n- Create Public Hosted Zone \u0026amp; NS Delegation Plan\n- ACM Certificates Automation\n- Test DNS\n- Link DNS to AWS Resources\n- Monitoring\n- Deploy Certificate Stack¬†2 weeks Structure Core Stack¬†Week 6,7 -¬†Initialize Core Stack for DynamoDB, S3, CloudFront, and OpenSearch infrastructure\n-¬†Create DynamoDB table with Provisioned Mode and Auto-Scaling for cost optimization\n- Implement Global Secondary Indexes for diverse access patterns\n- Configure KMS encryption and security settings for DynamoDB\n- Create all 4 S3 buckets (content, logs, incoming emails, CDN logs) with Intelligent-Tiering for cost optimization\n-¬†Configure CloudFront CDN with compression and caching optimization\n- Setup signed URLs for private content (avatars, backgrounds)\n- Create OpenSearch domain for advanced search with cost optimization\n- Create CoreStack class\n- Implement DynamoDB Single Table with cost optimization\n- Create 5 GSI indexes\n- Setup encryption and security for DynamoDB¬†- S3 Storage, CloudFront CDN,OpenSeach Domain\n- Deploy Core Stack\n2 week Authentication Stack Week 8 - Initialize Authentication Stack for Cognito User Pool\n-¬†Create Cognito User Pool with production-grade security settings\n- Setup SES for production email sending with Route 53 DNS automation\n- Implement Lambda triggers for Cognito lifecycle events\n- Cognito User Pool Setup\n- Implement Cognito User Pool with production settings\n- Configure SES email integration (Production mode)\n- Setup Cognito Lambda triggers\n1 week Backend Stack (API Gateway + Lambda ) Week 9,10 -¬†Create API Gateway REST API with production settings and Cognito authorizer\n- Configure Cognito User Pool authorizer for API Gateway\n- Enable API Gateway caching for production to improve performance and reduce Lambda invocations\n-¬†Enable request validation at API Gateway level to reject invalid requests early\n- Enable compression for API responses to reduce data transfer costs\n-¬†Configure API Gateway custom domain for Everyone Cook project\n- Setup API Router Lambda directory structure\n- Implement routing logic for API Gateway requests\n-¬†Deploy API Router Lambda to AWS and implement JWT validation for Cognito tokens\n- Setup Auth \u0026amp; User module directory structure\n- Create BackendStack class\n- Create API Gateway REST API\n- Setup Cognito Authorizer\n- Configure API Gateway caching\n-¬†Configure API Gateway request validation\n-¬†Enable API Gateway compression\n-¬†Configure API Gateway custom domain\n-¬†Create API Router Lambda structure\n-¬†Implement API Router handler\n-¬†Deploy API Router Lambda + Implement JWT Validation\n- Create Auth \u0026amp; User module structure\n- Implement authentication handlers\n- Implement user profile handlers,‚Ä¶\n- Social Module Lambda\n2 weeks Frontend Stack + Fix bug , Tester Week 11,12 - Foundation\n-¬†Authentication\n- UserProfile Managenment\n- Privacy Setting Pagess\n- Avatar Upload Component\n- Recipe Management\n- AI Recipe Suggestion\n- Social Feature, Search Discovery\n- Testing and Fix Bug\n- Functional profile view page\n- Login, signup, logout, password reset, and security middleware\n- Profile editing with validation\n- Avatar upload with S3 integration\n- Privacy settings interface\n2 week 3.2 OUT OF SCOPE Real-time Messaging / Chat System\nPrivate or group chat\nReal-time messaging infrastructure (WebSocket, SignalR, Firebase Realtime DB, etc.)\nMessage history storage \u0026amp; encryption\nFriends / Social Graph Management\nFriend requests, following/followers\nUser-to-user connection graph\nActivity feed, notifications tied to friend actions\nReal-time Voice \u0026amp; Video Calling\n1-to-1 or group voice call\nVideo call, screen sharing\nWebRTC signaling servers \u0026amp; TURN/STUN infrastructure\nAdvanced Social Interaction\nIn-app messaging reactions\nTyping indicators, online/offline status\nRead receipts, presence tracking\n3.3 PATH TO PRODUCTION 1. Project Foundation \u0026amp; Infrastructure\n- Initialize project structure\n- Set up core infrastructure baseline\n- Configure Route 53 Hosted Zone (DNS Stack)\n2. Cross-Region Certificate (Side Quest)\n- Handle CloudFront requirement for ACM certificate in us-east-1\n- Sync certificate usage with main stack in ap-southeast-1\n3. Core Application Stacks\n- Core Stack: Shared resources / environment setup\n- Authentication Stack: User auth, Cognito, permissions\n- Backend Stack: API Gateway + Lambda functions\n4. Frontend Deployment\n- Deploy frontend (S3 + CloudFront)\n- Bug fixes \u0026amp; QA testing\n4. EXPECTED AWS COST BREAKDOWN BY SERVICES Target workload: 100-500 Monthly Active Users (MAU)\nAverage Lambda duration: 200ms per invocation\nDynamoDB peak activity: ~8 hours per month\nS3 to CloudFront data transfer is FREE (same region)\nAll services leverage AWS Free Tier where applicable (Lambda 1M requests, SQS 1M requests, Cognito 50K MAU, Amplify 1000 build minutes)\nAPI Gateway caching enabled at 0.5GB ($14.60/month) - can be disabled to reduce costs\nCloudFront WAF removed to optimize costs (~$9/month savings), Shield Standard provides DDoS protection\nBedrock uses on-demand pricing with Claude 3 Haiku (lowest cost Anthropic model)\nhttps://calculator.aws/#/estimate?id=7a8833402a63e273357ddc71071bfc2cdce4be2c\n5. TEAM Partner Executive Sponsor\nName Title Description Email / Contact Info Nguyen Gia Hung Director of FCJ Vietnam Training Program As the Executive Sponsor, you are responsible for the overall oversight of the FCJ internship program. Ensure the project delivers learning value and adheres to AWS technical and career goals hunggia@amazon.com Project Stakeholders\nName Title Stakeholder for Email / Contact Info Van Hoang Kha Support Teams is the Executive Assistant responsible for overall oversight of the FCJ internship program Khab9thd@gmail.com Partner Project Team\nName Title Role Email / Contact Info Pham Minh Hoang Viet Leader Project Manager vietpmhse181851@gmail.com Nguyen Van Truong Member DevOps truongnvse182034@fpt.edu.vn Huynh Duc Anh Member Cloud Engineer anhhdse183114@fpt.edu.vn Nguyen Thanh Hong Member Tester hongntse183239@fpt.edu.vn Nguyen Quy Duc Member Frontend ducnqse182087@fpt.edu.vn Project Escalation Contacts\nName Title Role Email / Contact Info Pham Minh Hoang Viet Leader Project Manager vietpmhse181851@gmail.com 6. Resources \u0026amp; cost estimates Resource Responsibility Rate (USD) / Hour Solution Architects [[1]]{.mark} Architecture design, AWS service selection, security review, cost optimization $150 Engineers [[2]]{.mark} Frontend (Next.js), Backend (Lambda/Node.js), Infrastructure (CDK), Testing $100 Other . DevOps [1] CI/CD setup, monitoring, deployment automation $80 Project Phase Solution Architects Engineers Other\n(DevOps)\nTotal Hours Discovery \u0026amp; Requirements 16 24 8 48 Architecture Design 40 16 8 64 Development 16 200 40 256 Testing \u0026amp; QA 8 40 16 64 Deployment \u0026amp; Go-Live 8 24 24 56 Documentation \u0026amp; Training 8 16 8 32 Total Hours 96 320 104 520 Total Cost $14,400 | $32,000 $8,320 | $54,720 Monthly AWS Infrastructure Cost\nBased on AWS Pricing Calculator estimate for 100-500 MAU:\nService Monthly Cost (USD) Description Amazon DynamoDB $13.06 Single-table design, 5 GSIs, provisioned capacity Amazon S3 $0.84 2 buckets, Intelligent-Tiering Amazon CloudFront $1.44 CDN, Price Class 200 Amazon Cognito $5.00 User authentication AWS Lambda $0.00 13 functions (Free Tier) Amazon API Gateway $20.65 REST API with 0.5GB cache Amazon SQS $0.00 8 queues (Free Tier) Amazon SES $0.02 Transactional emails AWS KMS $2.00 2 customer managed keys AWS WAF $10.00 Web ACL, 5 rules Amazon CloudWatch $21.25 Metrics, dashboards, alarms, logs Amazon Route 53 $0.93 DNS hosted zone AWS Amplify $4.58 Frontend hosting (Next.js) Amazon Bedrock $64.80 Claude 3 Haiku AI Total ~$144.54 Per month Cost Summary\nCost Type Amount (USD) Notes One-time Development Cost $54,720 Resource hours √ó rates Monthly AWS Infrastructure ~$145 Based on 100-500 MAU Annual AWS Infrastructure ~$1,740 Monthly √ó 12 Year 1 Total Cost ~$56,460 Development + 12 months AWS Cost Contribution Distribution\nParty Contribution (USD) % Contribution of Total Customer $54,720 100% Partner $0 $0 AWS $0 $0 7. Acceptance Since this project is currently at the presentation stage and has not yet been formally evaluated by a customer, the following acceptance process is proposed for future delivery phases:\n7.1 Acceptance Criteria (Proposed) A deliverable will be considered acceptable when it meets the following criteria:\nFunctional features work as specified (authentication, recipe management, social features, AI functions).\nAll APIs respond correctly and integrate with AWS services (Lambda, API Gateway, DynamoDB, S3).\nSecurity requirements are met (JWT verification, HTTPS, access control, data encryption).\nUI works as expected on supported devices.\nNo critical errors appear during test execution.\n7.2 Acceptance Process Review period: 8 business days for evaluation and testing.\nIf accepted ‚Üí Deliverable is signed off.\nIf issues are found ‚Üí A rejection notice will be issued with feedback.\nFixes will be applied and a revised version will be resubmitted for review.\nIf no response is received by the end of the review period ‚Üí Deliverable is deemed accepted.\nAfter completing each milestone, the team submits the deliverables and documentation.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Secure Cloud Innovation Starts at re:Inforce 2025 Balancing Speed and Security Every day, I speak with security leaders who are facing an important balance. On one hand, their organizations are moving faster than ever, adopting breakthrough technologies like generative AI and expanding their cloud footprint. On the other hand, they need to maintain strong security controls and visibility in an increasingly complex environment.\nWe all know that simply adding more tools and security layers is not a sustainable path. We need a different approach that enables security to scale.\nre:Inforce 2025: A Security Roadmap that Accelerates Innovation This is the driving force behind our vision for AWS re:Inforce 2025. When implemented correctly, security at scale is not a barrier, but becomes a business enabler, helping organizations move faster and with greater confidence in the cloud.\nAt re:Inforce, we will share our perspective on simplifying security at scale, based on our experience supporting millions of customers worldwide. We will explore how organizations are building inherently resilient applications that both defend against modern threats and drive innovation. I am particularly excited to showcase real-world customer examples and reference architectures that demonstrate how security can directly support business objectives.\nAn Immersive Cloud Security Learning Environment We created re:Inforce as a specialized, in-person event for the security community. Unlike broad AWS events, re:Inforce provides the space to:\nDive deep into implementation details. Ask tough questions. Solve complex scenarios. Here, you can directly engage with the engineers who built AWS security services, collaborate with security partners, or schedule one-on-one sessions with AWS leaders to address your specific needs. This is the environment where true learning happens.\nWe have designed multiple learning paths to fit every stage of your cloud security journey. With more than 250 technical sessions, you will find content that matches your needs ‚Äî from automating security controls, aligning development and security teams, to transforming security operations.\nKey activities include:\nInteractive workshops where you can build solutions on the spot. Small-group technical chalk talks. Hands-on labs to experiment with new approaches. ‚ÄúSolution-building‚Äù sessions with AWS experts. Importantly, 70% of the content is advanced or expert level, ensuring the detailed implementation guidance you need.\nJoin Us at re:Inforce 2025 I invite you to join us for three days of experience that will transform how you think about and implement cloud security.\nRegister today with code SECBLObhZzr9 to receive a limited-time $300 USD discount. Hurry, as seats are going fast based on past experience.\nTogether, we will explore how simple, scalable cloud security can become the driving force for your organization‚Äôs future.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúDevOps on AWS Workshop‚Äù Event Objectives Introduce DevOps culture, principles, and best practices Provide hands-on knowledge of AWS DevOps services for CI/CD Demonstrate Infrastructure as Code (IaC) using CloudFormation and CDK Explore containerization and orchestration on AWS Present monitoring, observability, and incident response techniques Share real-world DevOps case studies and career direction Workshop Agenda \u0026amp; Key Highlights Morning Session (8:30 AM ‚Äì 12:00 PM) 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; DevOps Mindset Topics Recap of previous AI/ML session and its relationship with DevOps Introduction to DevOps culture, collaboration, and feedback loops Key DevOps metrics: DORA metrics (Lead Time, Deployment Frequency, Change Failure Rate, MTTR) Importance of continuous improvement and reliability Highlights:\nEmphasized DevOps as a cultural shift, not just technology Demonstrated how AI/ML workflows integrate into DevOps pipelines Clarified why DevOps improves speed, stability, and organizational alignment 9:00 ‚Äì 10:30 AM | AWS DevOps Services ‚Äì CI/CD Pipeline Core Components Source Control with AWS CodeCommit Git strategies: GitFlow, Trunk-based development, branch protection Build \u0026amp; Test with CodeBuild Buildspec configuration, unit tests, integration tests Deployments with CodeDeploy Deployment strategies: Blue/Green, Canary, Rolling updates Orchestration using AWS CodePipeline Automated multi-stage pipelines (Source ‚Üí Build ‚Üí Test ‚Üí Deploy) Demo End-to-end CI/CD pipeline from commit ‚Üí build ‚Üí deploy Blue/Green deployment with automated rollback Key takeaways:\nCI/CD improves release reliability and reduces manual effort Deployment strategies significantly reduce downtime and risks CodePipeline provides full automation with minimal operational overhead 10:30 ‚Äì 10:45 AM | Break 10:45 AM ‚Äì 12:00 PM | Infrastructure as Code (IaC) Topics Covered AWS CloudFormation Templates, stacks, drift detection, and stack lifecycle AWS CDK (Cloud Development Kit) High-level constructs, reusable patterns, multi-language support Side-by-side comparison Declarative (CloudFormation) vs programmatic (CDK) Demo Deploying infrastructure using CloudFormation templates Deploying the same architecture using AWS CDK Discussion: Choosing IaC Tools\nCloudFormation: stability, explicit control CDK: productivity, abstraction, reusability 12:00 ‚Äì 1:00 PM | Lunch Break (Self-arranged) Afternoon Session (1:00 PM ‚Äì 5:00 PM) 1:00 ‚Äì 2:30 PM | Container Services on AWS Topics Docker Fundamentals Microservices, container lifecycle, Dockerfile best practices Amazon ECR Image storage, vulnerability scanning, lifecycle policies Amazon ECS \u0026amp; Amazon EKS Cluster management, service scaling, orchestration patterns EC2 vs Fargate launch types AWS App Runner Simplified deployment for containerized applications Demo \u0026amp; Case Study Deploying microservices across ECS, EKS, and App Runner Comparison of complexity, cost, and developer experience Highlights:\nUnderstanding which container service to choose for each use case Importance of container scanning and image lifecycle management Real-world example of migrating monoliths to microservices 2:30 ‚Äì 2:45 PM | Break 2:45 ‚Äì 4:00 PM | Monitoring \u0026amp; Observability Topics Amazon CloudWatch Metrics, logs, alarms, dashboards, anomaly detection AWS X-Ray Distributed tracing and performance bottleneck analysis End-to-end observability workflow Collect ‚Üí Analyze ‚Üí Visualize ‚Üí Alert Demo Building a full-stack observability dashboard Tracing a request through microservices using X-Ray Best Practices\nAlert fatigue prevention Dashboards for SRE/DevOps teams Structured logs and correlation IDs 4:00 ‚Äì 4:45 PM | DevOps Best Practices \u0026amp; Case Studies Topics Deployment techniques: Feature flags, A/B testing, progressive delivery Testing automation: unit, integration, load testing in CI/CD Incident response and blameless postmortems Startup and enterprise DevOps transformation case studies Highlights:\nHow high-performance teams maintain reliability at scale Why postmortems drive continuous improvement Practical examples of real DevOps transformations on AWS 4:45 ‚Äì 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up Closing Items DevOps career pathways and required skill sets Recommended AWS certifications (Developer, SysOps, DevOps Engineer) Final discussion and networking Key Takeaways DevOps Culture Collaboration, automation, and continuous learning are essential DORA metrics provide clarity on performance and improvement areas Technical Skills CI/CD pipelines reduce deployment risks and shorten release cycles CloudFormation and CDK simplify infrastructure provisioning ECS, EKS, and App Runner enable scalable container deployment Observability ensures reliability and rapid incident response Applying to Work Adopt trunk-based development for faster release cycles Implement CI/CD pipelines with automated testing Use IaC to enforce consistency and reduce configuration drift Improve system reliability using monitoring dashboards and tracing tools Event Experience Learning from Experts Speakers shared valuable insights into DevOps culture and tooling Use cases and demos made the concepts practical and easy to apply Hands-on Understanding Walkthrough of CI/CD pipelines provided clarity on real-world workflows IaC demos highlighted differences between CloudFormation and CDK Container deployment comparison gave a clearer decision-making framework Networking and Discussion Opportunities to exchange experiences with other developers and engineers Discussions focused on automation, reliability, and scaling challenges Lessons Learned DevOps requires both cultural and technical adoption Monitoring and observability are essential for operating distributed systems Choosing the right container orchestration model depends on team maturity and workload "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS Virtual Private Cloud + VPC Security and Multi-VPC features + VPC - DirecConnect - LoadBalancer - ExtraResources - Practice: + Start with Amazon VPC and AWS VPN Site-to-Site + Subnets, Route table,Internet Gateway, Nat Gateway, Security Group, Network ACLs + Create VPC , VPC Resource Map, Create Subnet, Create an Internet Gateway, Create Route Table for Outbound Internet Routing via Internet Gateway, Create security groups + Create EC2 Instances in Subnets, Test connection, Create NAT Gateway, EC2 Instance Connect Endpoint 09/11/2025 09/16/2025 https://000009.awsstudygroup.com/ 3 - Set up Hybrid DNS with Route 53 Resolver + Generate Key Pair + Initialize CloudFormation Template + Configure Security Group + Connect to RDGW 09/18/2025 09/18/2025 https://000010.awsstudygroup.com/ 4 - Learn Hybrid DNS - Practice: + Set up DNS + Create Route 53 Outbound Endpoint + Create Route 53 Resolver Rules + Create Route 53 Inbound Endpoint + Test result + Clean up resources 09/19/2025 09/19/2025 https://000010.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types and AMIs + EBS volumes + Elastic IP + Security Groups and Key Pairs - SSH connection methods to EC2 - Launch templates, auto-scaling basics 09/20/2025 09/21/2025 https://000004.awsstudygroup.com/vi/ 6 - Practice: + Launch EC2 instances with different instance types + Connect via SSH and test key pairs + Attach and mount EBS volumes + Associate Elastic IPs + Test security group rules and connectivity 09/21/2025 09/21/2025 https://000004.awsstudygroup.com/vi/ Week 2 Achievements: Gained a clear understanding of AWS core service groups: Compute (EC2, Lambda, Auto Scaling) Storage (S3, EBS, EFS) Networking (VPC, Route 53, VPN, Security Groups) Successfully created and fully configured an AWS Free Tier account. Became familiar with the AWS Management Console: locating services, navigating dashboards, accessing features. Installed and configured AWS CLI with: Access Key \u0026amp; Secret Key Default Region Output format Performed essential operations via CLI: Check account information and configuration List available regions Launch and manage EC2 instances Create and manage key pairs Inspect running services and resources Practiced connecting and managing AWS resources through both the console and CLI in parallel. Learned practical networking and hybrid DNS setup with Route 53 Resolver. Successfully tested EC2 connectivity, NAT Gateway, and Elastic IP configurations. Developed foundational skills for automated infrastructure deployment with CloudFormation templates. Built confidence in troubleshooting common AWS networking and access issues. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Amazon Inspector Suppression Rules: Best Practices for AWS Organizations Delegated Administrator Setup Designate a single AWS account as the delegated administrator for Amazon Inspector. This account can centrally manage scans, aggregate findings, and apply suppression rules across all accounts in your AWS Organization. Benefits include:\nCentralized visibility of vulnerabilities Consistent policy enforcement Reduced administrative overhead Steps:\nEnable AWS Organizations and add member accounts. Assign the delegated admin role for Amazon Inspector. Verify access and permissions across accounts. Suppression Rules at Scale Suppression rules help filter out low-risk findings automatically, allowing security teams to focus on critical vulnerabilities. Key points:\nRules can be based on finding attributes, resource tags, or CVSS scores. Suppressed findings are archived, not deleted, ensuring auditability. Rules can be applied to multiple accounts using delegated admin. Example: Suppress findings for non-production EC2 instances tagged Environment:Dev with low CVSS scores (\u0026lt;4).\nTagging and Risk-Based Prioritization Using tags consistently across resources allows teams to prioritize vulnerabilities by business impact:\nTag by environment (Prod, Dev, Test) Tag by application owner Tag by compliance requirements Inspector can use these tags to generate reports and apply suppression selectively, aligning vulnerability management with business priorities.\nContinuous Review and Improvement Suppression rules and scan configurations are not ‚Äúset and forget.‚Äù Continuous monitoring ensures effectiveness:\nReview suppressed findings regularly to confirm rules remain valid. Adjust scan schedules, coverage, and scope to adapt to new services or workloads. Use aggregated reports from delegated admin account to identify trends and gaps. Benefits include:\nMaintaining compliance with security standards Reducing alert fatigue for security teams Ensuring timely remediation of critical vulnerabilities Multi-Account Considerations Managing suppression rules across multiple AWS accounts requires careful planning:\nUse AWS Organizations with delegated admin to avoid rule duplication. Apply inheritance patterns for member accounts. Combine tagging strategies with suppression rules to target specific accounts or workloads. This approach provides consistent security policies while enabling scalability and flexibility for growing organizations.\nConclusion By leveraging delegated admin, suppression rules, tagging, and continuous review, organizations can efficiently manage vulnerabilities at scale using Amazon Inspector. Following these best practices ensures that security teams focus on critical issues while maintaining auditability and compliance across all AWS accounts.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: ‚ÄúAWS Well-Architected Security Pillar Workshop‚Äù Workshop Objectives Join the course on the 5 security pillars of the AWS Well-Architected Framework, helping you build a strong security foundation and a comprehensive incident response process.\nWorkshop Agenda \u0026amp; Key Highlights Morning Session (8:30 AM ‚Äì 12:00 PM) 8:50 - 10h30 AM | Identity \u0026amp; Access Management,Detection Key Topics\nModern IAM architecture: Users, Roles, Policies Avoiding long-term credentials IAM Identity Center: SSO and permission sets SCP (Service Control Policies) and permission boundaries for multi-account MFA enforcement, credential rotation, Access Analyzer CloudTrail , GuardDuty, Security Hub Logging at every layer: VPC Flow Logs, ALB logs, S3 logs Alerting and automation with EventBridge Detection-as-Code (infrastructure + rules) Hands-on Demo\nIAM Policy validation Access simulation Key Takeaways\nLeast privilege reduces blast radius IAM Identity Center optimizes multi-account security SCPs enforce governance at the organization level 9:55 - 10:10 AM | Coffee Break Chill Toilet 10:45 - 11:30 AM | Infrastructure Protection Key Topics\nVPC segmentation: public vs private placement Security Groups vs NACLs: real-world use cases WAF, Shield, Network Firewall Workload protection: EC2, ECS/EKS security basics Design Exercise\nDesigning a secure multi-tier VPC architecture Key Takeaways\nDefense in depth: layered security Application protection against DDoS and OWASP Top 10 Centralized inspection with AWS Network Firewall 11h30 - 12:10 AM | Data Protection Key Topics\nAWS KMS: key policies, grants, rotation Encryption at-rest and in-transit: S3, EBS, RDS, DynamoDB Secrets Manager and Parameter Store: rotation models Data classification and access guardrails Hands-on Demo\nLarge-scale encryption deployment Automated secret rotation Key Takeaways\nEncryption must be applied both at-rest and in-transit Misconfigured key policies can cause outages Secrets Manager reduces risks of hardcoded credentials Summary \u0026amp; Q\u0026amp;A (11:40 AM - 12:00 PM) Summary Contents\nSummary of the 5 Security Pillar components Common pitfalls in Vietnamese enterprises Learning roadmap: Security Specialty, SA Pro Event Experience Security Foundation The AWS Shared Responsibility Model defines clear security boundaries Identity is the first and strongest line of defense Zero Trust Architecture is becoming the new standard Technical Skills IAM governance ensures scalable security KMS simplifies encryption across AWS services Network isolation reduces attack surface Security Hub + GuardDuty = continuous monitoring Applying to Work Enforce MFA and least privilege IAM Encrypt all critical data with KMS Enable GuardDuty, CloudTrail, Config by default Use multi-account architecture with Control Tower Shift-left security in CI/CD pipelines "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section will list and introduce the blogs you have translated. For example:\nBlog 1 - How AWS Protects Customers Against DDoS Attacks This blog describes how AWS detects and mitigates HTTP/2 rapid reset DDoS attacks with record-breaking traffic of over 155 million requests per second. AWS explains the attack mechanisms and highlights the role of services like CloudFront, Shield, WAF, and Route 53 in maintaining availability. Additionally, AWS emphasizes best practices and the combination of monitoring, threat intelligence, and global collaboration to protect customers against increasingly sophisticated threats.\nBlog 2 - Cloud Security Innovation at re:Inforce 2025 This blog introduces the AWS re:Inforce 2025 event, where security experts discuss how to balance innovation speed with system safety. You will learn how AWS shapes its security roadmap to help organizations move faster in the cloud, real-world customer examples, and an immersive learning environment with over 250 technical sessions, workshops, and labs. The article emphasizes the importance of building security at scale, simply and effectively, becoming a driving force for business innovation.\nBlog 3 - Best Practices for AWS Organizations This blog explains how to effectively manage vulnerabilities across multiple AWS accounts using Amazon Inspector. You will learn how to prioritize findings, apply tags for risk-based assessments, and use suppression rules to filter out less-critical findings. The article emphasizes building a culture of continuous vulnerability management and provides practical guidance for organizations.\nBlog 4 - Jackbox Games Opens New Opportunities with Amazon GameLift Streams This blog introduces Jackbox Games is a Chicago-based game studio renowned for party games like Quiplash, Fibbage, Drawful, Trivia Murder Party, and many others. With over 50 games in its catalog, Jackbox pursues a mission of \u0026ldquo;accessible gaming\u0026rdquo; enabling players to participate from any device they have. The company\u0026rsquo;s entire server infrastructure is built on Amazon Web Services.\nBlog 5 - Real-time Iceberg Ingestion with AWS DMS Modern data platforms increasingly require the ability to access fresh data with low latency to support real-time analytics and rapid decision-making. However, many organizations struggle to continuously ingest changes (insert, update, delete) from transactional databases into their data lakes while maintaining strong consistency guarantees.\nBlog 6 - Native SQL Server Replication Options on Amazon RDS Custom for SQL Server This blog introduces Modern applications often require reliable data distribution and high availability across multiple databases to support mission-critical business operations. SQL Server Replication enables database administrators (DBAs) to automatically distribute data across databases in near real-time, helping offload heavy read workloads from production systems and synchronizing data across regions or different database environments.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Learn and practice with Amazon VPC (basic networking, subnets, route tables, security groups). Get hands-on with EC2 instances: launch, connect via SSH, attach storage. Understand EBS, Elastic IP, and Security Group configurations. Combine AWS Console and CLI to manage EC2 networking and storage resources. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Read through prerequisites in lab guide - Understand AWS global infrastructure (Regions, AZs, VPC) - Review basic networking concepts (subnets, route tables, gateways) 09/22/2025 09/22/2025 https://000003.awsstudygroup.com/vi/ 3 - Learn how to create a custom VPC + Define CIDR block + Add public \u0026amp; private subnets + Configure route tables \u0026amp; internet gateway + Associate subnets 09/23/2025 09/23/2025 https://000004.awsstudygroup.com/vi/ 4 - Explore EC2 networking: + Security Groups (inbound \u0026amp; outbound rules) + Key pairs for SSH - Prepare environment for launching Linux EC2 instance 09/24/2025 09/24/2025 https://000004.awsstudygroup.com/vi/ 5 - Practice EC2 launch: + Choose AMI (Amazon Linux/Ubuntu) + Select instance type (t2.micro) + Configure networking (VPC, subnet, SG) + Assign Elastic IP - Test SSH connectivity 09/25/2025 09/25/2025 https://000004.awsstudygroup.com/vi/ 6 - Work with storage: + Create and attach additional EBS volume + Format and mount the volume - Verify instance status \u0026amp; connectivity via Console \u0026amp; CLI - Cleanup resources (terminate EC2, release Elastic IP, delete volumes) 09/26/2025 09/26/2025 https://000004.awsstudygroup.com/vi/ Week 3 Achievements: Understood AWS global infrastructure (Regions, AZs, and VPC). Successfully created a custom VPC with subnets, route tables, and an internet gateway. Learned to configure Security Groups and manage inbound/outbound rules. Launched a Linux EC2 instance with proper networking setup. Practiced connecting to EC2 via SSH using key pairs. Assigned and tested Elastic IP for stable public access. Attached, formatted, and mounted an EBS volume to extend storage. Verified instance status and resources using both AWS Console and CLI. Practiced proper cleanup: terminating EC2, releasing Elastic IP, and deleting EBS volumes. Built solid foundational skills for AWS networking (VPC) and compute (EC2). "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/3-blogstranslated/3.4-blog4/",
	"title": "Blog 4",
	"tags": [],
	"description": "",
	"content": "AWS for Games: Jackbox Games Opens New Opportunities with Amazon GameLift Streams By David Holladay March 6, 2025\nTags: Amazon CloudFront, Amazon GameLift, Amazon SQS, Amazon S3, Customer Solutions, Game Development, Games, Messaging, Networking \u0026amp; Content Delivery, Storage, Industries\nIntroduction Jackbox Games is a Chicago-based game studio renowned for party games like Quiplash, Fibbage, Drawful, Trivia Murder Party, and many others. With over 50 games in its catalog, Jackbox pursues a mission of \u0026ldquo;accessible gaming\u0026rdquo; enabling players to participate from any device they have. The company\u0026rsquo;s entire server infrastructure is built on Amazon Web Services.\nPreviously, players typically used a TV to display the game and a phone as a controller a simple but widely popular play style. Recently, Jackbox decided to partner with Amazon GameLift Streams to deliver installation-free, instant-access gaming experiences to players.\nAmazon GameLift Streams is a feature within GameLift that enables games to be streamed with low latency, high frame rates, and global scale helping studios like Jackbox expand their reach without depending on consoles or powerful devices.\n1. Cost Control While Scaling Bringing a game to a streaming platform can be expensive if done in isolation. Jackbox has a very large catalog, so they needed a way to optimize costs.\n1.1 Creating a Stream Group for All Games AWS partnered with Jackbox to consolidate all 50 games into a single Stream Group within Amazon GameLift Streams. This allows them to scale their entire catalog together rather than managing each game individually.\n1.2 Multi-tenancy \u0026amp; Reduced Resource Costs Because Jackbox games don\u0026rsquo;t require powerful GPUs, a single server instance can run multiple games simultaneously implementing a multi-tenancy model to save on costs.\n1.3 Using Data Channels to Insert Ads Amazon GameLift Streams provides a Data Channel feature that allows ads to be inserted at appropriate moments between game rounds helping Jackbox sustain a free-to-play model or support revenue through advertising.\n2. Improved Update and Release Processes 2.1 Traditional Party Pack Model \u0026amp; Limitations Previously, Jackbox released games in Party Packs each bundle containing 5 games. Updating a single game required updating the entire pack across all platforms, creating complexity in management and deployment.\n2.2 Streaming Enables Flexible, Individual Updates With GameLift Streams, Jackbox can update a single game on the server without affecting others, regardless of platform differences. This also allows players to seamlessly transition between games without closing and reopening bundles.\n2.3 Increased Game Discoverability When all games are accessible through the streaming service, players have more opportunities to try new games immediately rather than being limited to bundles they\u0026rsquo;ve previously purchased.\n3. Support \u0026amp; Deep Integration with AWS 3.1 Technical Support Integrated into Slack During deployment, AWS supported Jackbox directly through their Slack enabling the development team to communicate easily and resolve issues quickly. Jackbox\u0026rsquo;s CTO noted that this experience made AWS feel like part of their team.\n3.2 Using Multiple AWS Services Beyond GameLift Streams, Jackbox uses several AWS services to operate their system:\nAmazon EC2 for backend processing and session management Amazon S3 for storing game assets and data Amazon SQS for queue management Amazon CloudFront for fast and reliable content distribution All these services fit seamlessly within the AWS ecosystem Jackbox was already using.\n4. Future Direction \u0026amp; Vision Jackbox is excited about reaching new players through popular devices like Smart TVs without requiring a console. They envision an experience similar to opening a streaming app (like Netflix) to play games instantly.\nStreaming also enables Jackbox to explore partnerships with other studios, bringing games from different engines into their service expanding their catalog faster. Jackbox\u0026rsquo;s CTO emphasizes that streaming opens a \u0026ldquo;new business model\u0026rdquo; impossible with traditional Party Pack structures subscription, advertising, and direct access models and GameLift Streams provides the flexibility to continue innovating.\nConclusion Through Amazon GameLift Streams, Jackbox Games has achieved numerous benefits:\nExpanded global reach to players worldwide Reduced operational costs through multi-tenancy and centralized management Simplified game update processes Enabled flexible revenue models (advertising, subscription) Enhanced player experience and content discoverability The partnership between Jackbox and AWS demonstrates that game streaming is not just a trend, but a platform for innovation in the gaming industry.\nAbout the Author David Holladay David Holladay is a Principal Solutions Architect with the AWS Game Tech team, specializing in consulting and supporting game studios in building large-scale infrastructure on AWS. With over 15 years of experience in software development and game systems design, he has worked with many leading studios worldwide. When not working, David enjoys streaming newly released indie games and participating in game jam events to share programming expertise with the game development community.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Bedrock, Agent Core \u0026amp; Future of AI Agents\u0026rdquo; Event Overview A comprehensive workshop on AWS Bedrock and Agent Core, exploring the present and future of AI agents, AWS\u0026rsquo;s vision, and solutions for deploying agents to production efficiently.\nKey Highlights Part 1: The Future of AI Agents \u0026amp; AWS Vision Future of AI Agents\nTransition from experimental prototypes to production-ready solutions Enterprise adoption requires robust governance, security, and scalability Multi-agent orchestration and collaboration becoming standard Cost optimization and resource efficiency are critical challenges Part 2: AI Agent Models \u0026amp; Security Core Capabilities\nFoundation models: Claude, Titan, Llama with multi-modal support Security: data encryption, VPC endpoints, IAM policies, audit logging Challenge: Getting agents to production is still hard Part 3: AWS Bedrock Agent Core Core Components\nRuntime: Agent execution, context management, async/sync support Agent Gateway: Central entry point, request routing, authentication Memory: Short-term conversation memory, long-term knowledge store Browser/Web: Web scraping, data extraction, real-time retrieval Code Interpreter: Safe code execution (Python, Node.js), API integration Observability: Real-time tracking, performance metrics, cost insights Part 4: Diaflow - Business-Focused Agent Workflows Founder: Viet (Co-founder of Diaflow)\nUnique Value Proposition\nBusiness-first design Low-code development for rapid agent creation Enterprise integration with existing business systems Pre-built templates for common business workflows Cost efficiency through optimized resource utilization Key Differentiators\nFocus on ROI and business metrics Easier onboarding for non-technical stakeholders Domain-specific expertise and best practices Part 5: CloudThinker - AI Operations Management Core Focus: Cost \u0026amp; Risk Management\nCost Optimization\nResource utilization tracking and optimization Model selection based on cost/performance trade-offs Batch processing for non-real-time workloads Caching strategies to reduce API calls Budget controls and spending alerts Risk Management\nModel behavior monitoring and anomaly detection Fallback strategies and graceful degradation Guardrails and output validation Compliance and regulatory adherence Incident response and disaster recovery Key Benefits\nReduce operational costs by 30-50% Minimize production incidents and risks Improve agent reliability and consistency Enable confident large-scale deployments Event Photos "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/4-eventparticipated/",
	"title": "Events Attended",
	"tags": [],
	"description": "",
	"content": "During my internship, I had the opportunity to attend 4 events. Each event provided valuable knowledge, practical experience, and memorable moments.\nEvent 1 Event Name: AWS AI/ML \u0026amp; Generative AI\nTime: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Content:\nIntroduction to AWS AI/ML ecosystem Overview of key services such as Amazon SageMaker and Amazon Bedrock End-to-end ML model development and deployment workflow Introduction to Generative AI, Prompt Engineering, and RAG architecture Hands-on demos and expert-guided discussion Outcomes:\nAI/ML must be implemented with a clear strategy, not following trends RAG and Guardrails are essential for enterprise-grade GenAI solutions SageMaker and Bedrock significantly reduce time, cost, and accelerate innovation Event 2 Event Name: DevOps on AWS\nTime: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Content:\nIntroduction to DevOps culture, mindset, and principles Overview of DevOps services on AWS for CI/CD Hands-on Infrastructure as Code (IaC) using CloudFormation and CDK Understanding containerization and orchestration on AWS Overview of monitoring, observability, and incident response Real-world DevOps case studies and career guidance Outcomes:\nDevOps is a combination of culture and technology Observability is the foundation of stable operations Choosing the right orchestrator depends on team capabilities and workload Event 3 Event Name: AWS Security Best Practices\nTime: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent Content:\nImplement MFA and apply IAM least privilege Encrypt all critical data using AWS KMS Enable GuardDuty, CloudTrail, and Config by default Adopt a multi-account architecture with AWS Control Tower Outcomes:\nMisconfiguration is the #1 cause of security incidents Security must be continuous and automated Zero Trust is becoming the new security standard Event 4 Event Name: AWS Bedrock, Agent Core \u0026amp; Future of AI Agents\nTime: December 5, 2025\nRole: Attendee\nEvent Content:\nComprehensive workshop on AWS Bedrock and Agent Core Exploring the present and future of AI agents AWS\u0026rsquo;s vision for production-ready AI solutions Foundation models: Claude, Titan, Llama with multi-modal support AWS Bedrock Agent Core components (Runtime, Gateway, Memory, Browser, Code Interpreter, Observability) Diaflow: Business-focused agent workflow platform CloudThinker: AI operations management and cost optimization Outcomes:\nUnderstanding Bedrock Agent Core architecture is essential for production AI deployments Cost optimization and risk management are critical for enterprise AI solutions Business-focused approaches (like Diaflow) outperform pure technical implementations Operations management tools enable confident large-scale AI deployments "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Understand and practice with Amazon S3 (static website hosting, public access, versioning, CloudFront). Understand and practice with Amazon RDS (create DB, backup/restore, VPC/Subnet/SG configuration, cleanup). Compare using AWS Console vs AWS CLI for managing resources. Tasks \u0026amp; Progress: Day Task Start Date Completion Date Reference Material 2 - Review S3 lab overview - Understand steps: create bucket, enable static website, configure public access, versioning, cleanup - Prepare AWS Free Tier account for practice 29/09/2025 29/09/2025 https://000057.awsstudygroup.com/vi/ 3 - Started with Amazon S3 + Create S3 bucket + Upload objects + Enable static website hosting + Configure Block Public Access \u0026amp; object permissions + Test website + Configure CloudFront + Enable versioning + Copy objects to another region + Cleanup resources 30/09/2025 30/09/2025 https://000057.awsstudygroup.com/vi/ 4 - Review RDS lab overview - Learn steps: create VPC, Subnet, Security Groups, DB Subnet Group, RDS instance, backup/restore, cleanup 01/10/2025 01/10/2025 https://000005.awsstudygroup.com/vi/ 5 - Started with Amazon RDS + Create VPC + Create EC2 \u0026amp; RDS security groups + Create DB Subnet Group + Launch RDS DB instance + Backup \u0026amp; restore DB + Test DB endpoint + Cleanup resources 02/10/2025 02/10/2025 https://000005.awsstudygroup.com/vi/ 6 - Practice AWS CLI commands for S3 and RDS: + Create and list S3 buckets via CLI + Upload/download objects via CLI + Describe and connect to RDS instance via CLI - Document differences between Console and CLI workflows 03/10/2025 03/10/2025 https://000057.awsstudygroup.com/vi/, https://000005.awsstudygroup.com/vi/ Week Achievements: Successfully hosted a static website on Amazon S3 with CloudFront and versioning. Learned to manage object permissions and configure public access in S3. Gained hands-on experience creating and managing Amazon RDS DB instances with networking setup (VPC, Subnets, Security Groups). Practiced backup \u0026amp; restore operations for RDS. Cleaned up all resources properly to avoid unnecessary costs. Compared AWS Console vs AWS CLI workflows and noted pros/cons for each approach. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/3-blogstranslated/3.5-blog5/",
	"title": "Blog 5",
	"tags": [],
	"description": "",
	"content": "AWS for Database: Real-time Iceberg Ingestion with AWS DMS By Caius Brindescu ‚Äì June 4, 2025\nTags: AWS Database, AWS DMS, Apache Iceberg, Data \u0026amp; Analytics, Industries\nIntroduction Modern data platforms increasingly require the ability to access fresh data with low latency to support real-time analytics and rapid decision-making. However, many organizations struggle to continuously ingest changes (insert, update, delete) from transactional databases into their data lakes while maintaining strong consistency guarantees.\nApache Iceberg is an open table format that brings ACID transactions, schema evolution, time-travel queries, and multi-engine interoperability.\nIn this post (guest-authored by Caius Brindescu in collaboration with AWS), we discuss how to build a near real-time data pipeline using AWS Database Migration Service (DMS) to capture change data (CDC) from SQL databases and ingest it into Iceberg tables‚Äîensuring consistency, fault tolerance, and seamless integration with downstream systems like Snowflake.\nAmazon Web Services, Inc.\n1. What is AWS DMS and Why It Matters AWS Database Migration Service (DMS) is a managed service that migrates and synchronizes data across heterogeneous database engines.\nDMS supports two primary modes:\nFull Load ‚Äì initial bulk copy from source to destination CDC (Change Data Capture) ‚Äì captures inserts, updates, and deletes in near real-time Combining Full Load + CDC allows Iceberg tables to stay continuously updated with latency typically below 5 seconds.\nAmazon Web Services, Inc.\n2. Pipeline Architecture: SQL ‚Üí AWS DMS ‚Üí Iceberg 2.1 High-Level Architecture A typical pipeline looks like:\nSource DB (MySQL/PostgreSQL) ‚Üí AWS DMS ‚Üí Iceberg Table on S3 ‚Üí Downstream Analytics (Snowflake, Athena, Spark)\nThe transactional database acts as the origin point of change events. DMS handles the full load + CDC, and Iceberg stores the data in a scalable open table format.\nAmazon Web Services, Inc.\n2.2 Components and Roles Component Role Source Database Generates insert/update/delete events AWS DMS Performs full load + CDC and streams data Iceberg Table on S3 ACID table storage with multi-engine access Merge/Upsert Logic Applies CDC changes to the table Downstream Systems Snowflake, Athena, Spark for analytics 3. Step-by-Step Setup 3.1 Creating DMS Endpoints Source Endpoint: Connects to SQL database (MySQL/PostgreSQL) Target Endpoint: Points to S3 location storing Iceberg data Sample configuration includes hostname, port, username, and password.\n3.2 Creating a Replication Task Select Full Load + CDC mode\nChoose specific tables\nSet small batch sizes for low latency\nEnable large object options if necessary (LOB/JSON)\nWhen executed, DMS will:\nPerform initial full load Continuously read source logs Send changes to target staging 3.3 Handling DMS Output and Writing to Iceberg DMS writes change records (often JSON/CSV) into staging, including an action column.\nExample CDC staging:\nid name action 1 Alice INSERT 2 Bob UPDATE 3 Carol DELETE Apply changes to Iceberg using a MERGE statement:\nMERGE INTO iceberg_table t USING staging_table s ON t.id = s.id WHEN MATCHED AND s.action = \u0026#39;DELETE\u0026#39; THEN DELETE WHEN MATCHED THEN UPDATE SET name = s.name WHEN NOT MATCHED THEN INSERT (id, name) VALUES (s.id, s.name); Conclusion Building a real-time data ingestion pipeline with AWS DMS and Apache Iceberg enables organizations to keep analytical datasets fresh, consistent, and scalable. By combining full-load initialization with continuous CDC streaming, teams can ensure low-latency updates while benefiting from Iceberg‚Äôs ACID guarantees and multi-engine interoperability. This architecture not only improves decision-making but also provides a future-proof foundation for modern analytics workloads.\nAbout the Author Caius Brindescu Caius Brindescu l√† K·ªπ s∆∞ ch√≠nh (Principal Engineer) t·∫°i Etleap, chuy√™n tri·ªÉn khai c√°c gi·∫£i ph√°p d·ªØ li·ªáu th·ªùi gian th·ª±c. √îng h·ª£p t√°c v·ªõi AWS ƒë·ªÉ x√¢y d·ª±ng c√°c pipeline s·ª≠ d·ª•ng AWS DMS v√† Apache Iceberg, h∆∞·ªõng ƒë·∫øn vi·ªác gi√∫p doanh nghi·ªáp ƒë·∫°t kh·∫£ nƒÉng x·ª≠ l√Ω d·ªØ li·ªáu g·∫ßn real-time v√† m·ªü r·ªông hi·ªáu qu·∫£.\nMahesh Kansara Mahesh l√† qu·∫£n l√Ω k·ªπ thu·∫≠t c∆° s·ªü d·ªØ li·ªáu t·∫°i Amazon Web Services (AWS). Anh l√†m vi·ªác ch·∫∑t ch·∫Ω v·ªõi c√°c nh√≥m ph√°t tri·ªÉn v√† k·ªπ s∆∞ ƒë·ªÉ c·∫£i thi·ªán c√°c d·ªãch v·ª• di tr√∫ v√† sao ch√©p d·ªØ li·ªáu. ƒê·ªìng th·ªùi, anh c≈©ng h·ª£p t√°c v·ªõi kh√°ch h√†ng ƒë·ªÉ cung c·∫•p h∆∞·ªõng d·∫´n v√† h·ªó tr·ª£ k·ªπ thu·∫≠t cho nhi·ªÅu d·ª± √°n c∆° s·ªü d·ªØ li·ªáu v√† ph√¢n t√≠ch, gi√∫p h·ªç n√¢ng cao gi√° tr·ªã c·ªßa c√°c gi·∫£i ph√°p khi s·ª≠ d·ª•ng AWS.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. {\r\u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;,\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;,\r\u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34;\r],\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Understand how to deploy and manage a WordPress application on AWS using EC2, RDS, and S3. Learn how to distribute content globally using Amazon CloudFront. Gain experience deploying and managing AWS Managed Microsoft AD. Configure IAM federation and user authentication with AWS Managed AD. Tasks \u0026amp; Progress: Day Task Start Date Completion Date Reference Material 2 - Review lab overview for WordPress deployment - Understand components: EC2, RDS, S3, Security Groups - Prepare key pair and IAM role for WordPress instance 06/10/2025 06/10/2025 https://000101.awsstudygroup.com/ 3 - Deploy WordPress on AWS Cloud + Launch EC2 and configure Apache/PHP + Create RDS MySQL database + Connect WordPress to RDS + Test website accessibility + Configure S3 for media storage + Cleanup resources 07/10/2025 07/10/2025 https://000101.awsstudygroup.com/ 4 - Review AWS CloudFront Workshop - Configure CloudFront distribution for WordPress static content - Enable caching, HTTPS, and custom domain setup - Test website performance improvement 08/10/2025 08/10/2025 https://000130.awsstudygroup.com/ 5 - Review Deploy AWS Managed AD lab - Understand steps: create VPC, subnets, and directory configuration - Deploy AWS Managed AD and test domain join from EC2 09/10/2025 09/10/2025 https://000093.awsstudygroup.com/ 6 - Practice IAM Federation with AWS Managed AD + Configure trust relationships + Integrate IAM users and AD authentication + Verify access via AWS Console and AD credentials + Cleanup all resources 10/10/2025 10/10/2025 https://000095.awsstudygroup.com/ Week Achievements: Successfully deployed WordPress website on AWS with RDS and S3 integration. Configured CloudFront distribution to accelerate global content delivery. Deployed and tested AWS Managed Microsoft AD within a secure VPC. Implemented IAM Federation with AD for centralized identity management. Completed cleanup of all resources and documented step-by-step deployment process. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/3-blogstranslated/3.6-blog6/",
	"title": "Blog 6",
	"tags": [],
	"description": "",
	"content": "AWS for Database : Native SQL Server Replication Options on Amazon RDS Custom for SQL Server (AWS Database, Amazon RDS Custom, SQL Server, Replication, Data \u0026amp; Analytics, Industries)\nIntroduction Modern applications often require reliable data distribution and high availability across multiple databases to support mission-critical business operations. SQL Server Replication enables database administrators (DBAs) to automatically distribute data across databases in near real-time, helping offload heavy read workloads from production systems and synchronizing data across regions or different database environments.\nAmazon RDS Custom for SQL Server is a managed service that combines the flexibility of a self-managed database with the automation benefits of a fully managed offering. With RDS Custom, you can leverage native SQL Server replication features while AWS handles infrastructure tasks such as backups, patching, snapshots, and monitoring.\nThis blog explores how to configure SQL Server Replication on RDS Custom, including supported replication types (snapshot, transactional, merge, peer-to-peer) and step-by-step instructions to configure distributors, publications, and subscriptions.\nAmazon Web Services, Inc.\n1. Supported Replication Types \u0026amp; Comparison Table 1.1 Roles \u0026amp; Capabilities of RDS Custom Role / Replication Mode RDS for SQL Server RDS Custom for SQL Server Publisher No Yes Distributor No Yes Pull Subscriber No Yes Push Subscriber Yes Yes RDS Custom extends the capabilities beyond standard RDS by supporting roles such as Publisher and Distributor, which are typically disallowed on regular RDS instances.\nAmazon Web Services, Inc.\n1.2 SQL Server Replication Types Replication Type Supported on RDS for SQL Server Supported on RDS Custom Notes Transactional Yes (as subscriber only) Yes Common for near real-time synchronization ‚Äî Amazon Web Services, Inc. Snapshot Yes (as subscriber only) Yes Captures a full data state at a specific moment ‚Äî Amazon Web Services, Inc. Merge No Yes (with limitations) Requires hostname instead of CNAME when host replacement occurs ‚Äî Amazon Web Services, Inc. Peer-to-peer No Yes (with limitations) Needs hostname configuration to avoid host replacement issues ‚Äî Amazon Web Services, Inc. Note: Merge and peer-to-peer replication on RDS Custom require using hostnames instead of CNAMEs to prevent issues during host replacement.\nAmazon Web Services, Inc.\n2. Solution Overview \u0026amp; Architecture The original solution showcases a sample architecture with two RDS Custom instances in the same region. One instance acts as both Publisher and Distributor, while the second instance serves as the Subscriber. Replication is handled through publications and subscriptions.\nAmazon Web Services, Inc.\nBefore configuring replication, ensure:\nBoth RDS Custom instances are deployed with proper networking and domain setup (if using Kerberos).\nAmazon Web Services, Inc.\nThe server name (@@SERVERNAME) matches the DNS hostname used for replication.\nAmazon Web Services, Inc.\nSQL Server Agent is enabled on both the publisher and subscriber.\nAmazon Web Services, Inc.\nNetwork connectivity (port 1433) between instances is properly configured.\nAmazon Web Services, Inc.\n3. Transactional Replication Configuration Guide Below are the steps to set up transactional replication between RDS Custom for SQL Server instances.\nAmazon Web Services, Inc.\nStep 1: Update the Server Name (SERVERNAME) if Needed Connect to each instance (publisher and subscriber) and check the current value of @@SERVERNAME.\nIf the current server name does not match the hostname/CNAME you plan to use, execute:\nSELECT @@SERVERNAME AS \u0026#39;Current_Server_Name\u0026#39;; EXEC sp_dropserver \u0026#39;old_server_name\u0026#39;; EXEC sp_addserver \u0026#39;new_cname\u0026#39;, \u0026#39;local\u0026#39;; Step 2: Create the Publication on the Publisher: CREATE DATABASE PublicationDB; GO USE PublicationDB; GO CREATE TABLE Tracker ( Date DATE, RPO TIME, Status VARCHAR(255), PRIMARY KEY (RPO) ); GO INSERT INTO Tracker VALUES (CAST(GETDATE() AS Date), CAST(GETDATE() AS time(7)), \u0026#39;Success\u0026#39;); GO On the Subscriber: CREATE DATABASE SubscriptionDB; GO Step 3: Create the Publication and Subscription On the Publisher, open Replication \u0026gt; New Publication Wizard Select the PublicationDB database and choose the Tracker table as the article reate a subscription pointing to the Subscriber instance Choose a push subscription, with the agent running on the Distributor (which can be hosted on the same instance as the publisher if configured) After setup, you can monitor replication activity using Replication Monitor. 4. Advantages, Limitations \u0026amp; Considerations Advantages Supports native SQL Server replication on RDS Custom ‚Äî no need to build an external replication system Provides multiple replication types (transactional, snapshot, merge, peer-to-peer) to meet different use cases Helps offload analytical workloads from the production system Limitations \u0026amp; Risks Merge and peer-to-peer replication require hostname configuration and may encounter issues during host replacement Latency may occur under heavy transactional workloads Modifying the server name (@@SERVERNAME) requires a restart and must be done carefully Replication agents (Snapshot, Log Reader, Distribution) must be monitored to avoid failures Practical Considerations Ensure DNS/hostname stability ‚Äî do not rely solely on CNAMEs if host replacement may occur Confirm SQL Server Agent is configured for auto-start Configure appropriate Windows or domain accounts for replication agents (Snapshot Agent, Log Reader Agent, Distributor Agent) Monitor latency and replication errors through SQL Server Replication Monitor Conclusion Amazon RDS Custom for SQL Server allows you to use native SQL Server replication features that are not supported on standard RDS instances. You can deploy snapshot, transactional, merge, or peer-to-peer replication depending on your needs, with special attention to hostname configuration, server name settings, agent permissions, and network access.\nFor environments requiring data distribution across multiple databases, high availability, or separate analytical workloads, native replication on RDS Custom provides powerful benefits without the need to build an external replication solution.\nAbout the Author Sudhir Amin Sudhir l√† Ki·∫øn tr√∫c s∆∞ gi·∫£i ph√°p cao c·∫•p (Senior Solutions Architect) t·∫°i Amazon Web Services, l√†m vi·ªác t·∫°i New York. Anh cung c·∫•p h∆∞·ªõng d·∫´n ki·∫øn tr√∫c v√† h·ªó tr·ª£ k·ªπ thu·∫≠t cho c√°c kh√°ch h√†ng doanh nghi·ªáp tr√™n nhi·ªÅu lƒ©nh v·ª±c kh√°c nhau, gi√∫p h·ªç ƒë·∫©y nhanh qu√° tr√¨nh chuy·ªÉn ƒë·ªïi v√† ·ª©ng d·ª•ng ƒëi·ªán to√°n ƒë√°m m√¢y. Ngo√†i c√¥ng vi·ªác, Sudhir ƒëam m√™ bi-a (snooker) v√† c√°c m√¥n th·ªÉ thao ƒë·ªëi kh√°ng nh∆∞ boxing v√† UFC. Anh c≈©ng th√≠ch du l·ªãch ƒë·∫øn c√°c khu b·∫£o t·ªìn thi√™n nhi√™n tr√™n kh·∫Øp th·∫ø gi·ªõi ƒë·ªÉ quan s√°t c√°c lo√†i ƒë·ªông v·∫≠t hoang d√£ trong m√¥i tr∆∞·ªùng t·ª± nhi√™n c·ªßa ch√∫ng.\nMesgana Gormley Mesgana l√† Ki·∫øn tr√∫c s∆∞ gi·∫£i ph√°p chuy√™n gia c∆° s·ªü d·ªØ li·ªáu cao c·∫•p (Senior Database Specialist Solution Architect) thu·ªôc b·ªô ph·∫≠n Worldwide Public Sector (WWPS) t·∫°i Amazon Web Services (AWS), l√†m vi·ªác c√πng nh√≥m Amazon RDS. C√¥ t·∫≠p trung v√†o vi·ªác cung c·∫•p h∆∞·ªõng d·∫´n k·ªπ thu·∫≠t cho kh√°ch h√†ng AWS, h·ªó tr·ª£ h·ªç di chuy·ªÉn, thi·∫øt k·∫ø, tri·ªÉn khai v√† t·ªëi ∆∞u h√≥a c√°c kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác c∆° s·ªü d·ªØ li·ªáu quan h·ªá tr√™n AWS m·ªôt c√°ch hi·ªáu qu·∫£. Ngo√†i c√¥ng vi·ªác, Mesgana y√™u th√≠ch du l·ªãch v√† d√†nh th·ªùi gian b√™n gia ƒë√¨nh, b·∫°n b√®.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn how to design and deploy serverless architectures using AWS services. Build and test applications with AWS Lambda, API Gateway, and DynamoDB. Automate infrastructure deployment using AWS CloudFormation. Explore event-driven design and cross-service integration. Apply monitoring and logging best practices for serverless environments. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study Serverless Computing concepts and benefits. - Understand when to use Lambda vs EC2. - Practice: + Create a simple Lambda function in Python. + Test function triggers via AWS Console and CLI. 13/10/2025 13/10/2025 AWS Study Group 3 - Learn how to build APIs with Amazon API Gateway. - Practice: + Create a REST API endpoint linked to Lambda. + Deploy API stages and test public access. 14/10/2025 14/10/2025 AWS Study Group 4 - Work with Amazon DynamoDB for serverless data storage. - Practice: + Create a DynamoDB table and define partition/sort keys. + Integrate Lambda to perform CRUD operations. + Test data retrieval and updates via API Gateway. 15/10/2025 15/10/2025 AWS Study Group 5 - Learn AWS CloudFormation basics for Infrastructure as Code (IaC). - Practice: + Write a YAML template to deploy Lambda, API Gateway, and DynamoDB. + Automate stack creation and deletion. 16/10/2025 16/10/2025 AWS Study Group 6 - Implement monitoring and automation for serverless systems. - Practice: + Use CloudWatch Logs to track Lambda execution. + Set up CloudWatch Alarms for errors and throttles. + Create a simple automation rule with EventBridge. 17/10/2025 17/10/2025 AWS Study Group Week 6 Achievements: Serverless Fundamentals\nUnderstood the benefits of serverless design (no server management, auto-scaling, pay-per-use).\nCreated and tested basic Lambda functions triggered manually and via API events.\nAPI Gateway Integration\nBuilt a REST API with Amazon API Gateway connected to Lambda.\nDeployed multiple stages (dev, prod) and tested public access successfully.\nDynamoDB Integration\nDesigned a DynamoDB table with efficient key structure for fast access.\nIntegrated Lambda to perform CRUD operations directly from serverless functions.\nTested full API-Lambda-DynamoDB workflow end-to-end.\nInfrastructure as Code (IaC)\nAutomated deployment of serverless architecture using CloudFormation templates.\nManaged stack creation, updates, and teardown in a reproducible way.\nMonitoring and Automation\nImplemented CloudWatch Logs and Alarms to detect runtime issues.\nUsed EventBridge to trigger alerts or workflows based on Lambda events.\nStrengthened observability, reliability, and automation in the serverless environment.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": " Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Understand the concepts and benefits of containerization in application development. Learn how to build, manage, and deploy Docker containers. Explore Amazon ECS (Elastic Container Service) and ECR (Elastic Container Registry). Deploy a containerized web application using ECS Fargate. Introduce CI/CD automation for container deployment pipelines. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review the difference between Containers and Virtual Machines. - Study the role of Docker in application packaging and isolation. - Practice: + Install Docker Desktop. + Create a Dockerfile for a simple web app (Node.js or Python Flask). + Build and run the container locally. 20/10/2025 20/10/2025 AWS Study Group 3 - Learn about Amazon Elastic Container Registry (ECR). - Practice: + Create an ECR repository. + Tag and push the Docker image to ECR. + Verify image storage and permissions. 21/10/2025 21/10/2025 AWS Study Group 4 - Explore Amazon Elastic Container Service (ECS) basics. - Practice: + Create an ECS Cluster using Fargate launch type. + Define Task Definitions and Services. + Deploy the containerized app. 22/10/2025 22/10/2025 AWS Study Group 5 - Integrate Load Balancing with ECS. - Practice: + Configure Application Load Balancer (ALB). + Connect ECS service to ALB for public access. + Test high availability and scaling. 23/10/2025 23/10/2025 AWS Study Group 6 - Automate deployments using AWS CodePipeline and CodeBuild. - Practice: + Set up CI/CD pipeline to build Docker images. + Deploy updates automatically to ECS. + Test zero-downtime deployment. 24/10/2025 24/10/2025 AWS Study Group Week 7 Achievements: Docker Fundamentals\nUnderstood how containers differ from virtual machines and why they are lightweight. Created and tested Docker containers locally using custom Dockerfiles. ECR Image Management\nCreated and managed private ECR repositories. Successfully pushed and pulled Docker images from AWS ECR. ECS Deployment\nBuilt and deployed containerized applications using ECS Fargate. Configured task definitions, services, and networking settings for ECS workloads. Load Balancing and Scaling\nIntegrated Application Load Balancer (ALB) with ECS services for public access. Verified automatic scaling and traffic distribution between containers. CI/CD Automation\nImplemented a simple CI/CD pipeline using CodePipeline and CodeBuild. Automated image build and deployment process to ECS. Achieved efficient, repeatable, and zero-downtime container deployments. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Learn advanced AWS networking concepts including VPC Peering, Transit Gateway, and PrivateLink. Implement hybrid cloud connectivity between on-premises and AWS. Understand and configure AWS Direct Connect for dedicated network connections. Practice network security best practices and traffic flow management. Monitor and troubleshoot network performance using VPC Flow Logs and CloudWatch. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study VPC Peering and Transit Gateway concepts. - Practice: + Create VPC peering connections between two VPCs. + Configure routing tables for traffic flow. + Test connectivity using EC2 instances. 27/10/2025 27/10/2025 AWS Study Group 3 - Learn AWS PrivateLink and endpoint services. - Practice: + Create a PrivateLink endpoint to access a service privately. + Test service access without using the public internet. 28/10/2025 28/10/2025 AWS Study Group 4 - Explore AWS Direct Connect for dedicated network connections. - Practice: + Create a Direct Connect connection. + Set up a virtual interface and verify routing. 29/10/2025 29/10/2025 AWS Study Group 5 - Learn hybrid cloud design and integration with on-premises environments. - Practice: + Connect an on-premises network to AWS via VPN and Direct Connect. + Test failover and routing between cloud and local resources. 30/10/2025 30/10/2025 AWS Study Group 6 - Implement network monitoring and security. - Practice: + Enable VPC Flow Logs and analyze traffic. + Set up CloudWatch Alarms for network anomalies. + Apply Security Group and NACL best practices. 31/10/2025 31/10/2025 AWS Study Group Week 8 Achievements: VPC Peering \u0026amp; Transit Gateway\nSuccessfully established VPC peering connections and tested cross-VPC traffic.\nConfigured Transit Gateway to centralize routing across multiple VPCs.\nAWS PrivateLink\nCreated PrivateLink endpoints for secure, private access to AWS services.\nVerified that traffic bypassed the public internet, improving security.\nAWS Direct Connect\nConfigured dedicated network connections using Direct Connect.\nTested routing and bandwidth improvements for hybrid connectivity.\nHybrid Cloud Integration\nConnected on-premises network with AWS using VPN and Direct Connect.\nTested failover, routing, and hybrid communication reliability.\nNetwork Monitoring \u0026amp; Security\nEnabled VPC Flow Logs and analyzed traffic patterns.\nSet up CloudWatch Alarms for network performance anomalies.\nApplied Security Groups and NACLs to enforce best practices.\n"
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Learn how to design secure and scalable CI/CD pipelines. Build deployment workflows using AWS CodePipeline, CodeBuild, and CodeDeploy. Integrate GitHub or CodeCommit as the source provider. Automate application deployment to EC2 and Lambda. Strengthen pipeline security and add automated testing stages. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study concepts of CI/CD pipelines on AWS.\n- Understand how CodePipeline orchestrates build ‚Üí test ‚Üí deploy.\n- Practice: + Create a simple CodePipeline with GitHub as the source.\n03/11/2025 03/11/2025 AWS Study Group 3 - Learn AWS CodeBuild for building and testing applications.\n- Practice: + Write a buildspec.yml file.\n+ Run automated builds and unit tests.\n04/11/2025 04/11/2025 AWS Study Group 4 - Work with AWS CodeDeploy for deployment automation.\n- Practice: + Create an EC2 deployment group.\n+ Deploy application revisions using AppSpec.\n+ Test in-place and blue/green deployments.\n05/11/2025 05/11/2025 AWS Study Group 5 - Integrate Lambda deployments into CodePipeline.\n- Practice: + Deploy Lambda functions using CodeDeploy Lambda.\n+ Test linear and canary deployment strategies.\n06/11/2025 06/11/2025 AWS Study Group 6 - Add pipeline security and monitoring.\n- Practice: + Enable CloudWatch and SNS notifications.\n+ Configure IAM permissions for pipeline stages.\n+ Add test/approval stages for safer deployments.\n07/11/2025 07/11/2025 AWS Study Group Week 9 Achievements: CI/CD Pipeline Fundamentals\nUnderstood the CI/CD workflow: source ‚Üí build ‚Üí test ‚Üí deploy. Built the first automated CodePipeline integrated with GitHub. Build Automation (CodeBuild)\nWrote a complete buildspec.yml for automated build and testing. Successfully executed builds in an isolated CodeBuild environment. Deployment Automation (CodeDeploy)\nConfigured EC2 deployment groups and executed In-place \u0026amp; Blue/Green deployments. Understood the role of AppSpec and lifecycle hooks. Serverless Deployment\nAutomated deployment of Lambda functions using CodeDeploy. Tested safe rollout strategies such as Linear \u0026amp; Canary deployments. Pipeline Security \u0026amp; Governance\nAdded approval steps, IAM permissions, and fine-grained access controls. Monitored pipeline executions using CloudWatch \u0026amp; SNS alerts. Improved reliability and safety of the overall CI/CD workflow. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Learn to work with virtual machines using VMWare and migrate them to AWS. Practice managing S3 buckets, permissions, static website hosting, and CloudFront. Set up and work with AWS Storage Gateway, file shares, and Multi-AZ file systems. Improve skills in monitoring, performance testing, scaling storage, and cleaning up cloud resources. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - VMWare Workstation + Export Virtual Machine from On-premises + Upload virtual machine on AWS + Import virtual machine to AWS + Deploy Instance from AMI 17/09/2025 17/09/2025 AWS Study Group 3 - Setting up S3 bucket ACL - Export virtual machine from instance\n+ Resource Cleanup on AWS Cloud + Create Stroage Gateway + Create Files Shares + Mount File shares on On-premises 18/09/2025 18/09/2025 AWS Study Group 4 - Practice: + Create an HDD Muliti-AZ file system + Creaye new file share + Test Performance + Monitor Performance + Enable data deduplication + Manage user sessions and open files + Scale throughtout capacity,storage capacity + Delete enviroment 19/09/2025 19/09/2025 AWS Study Group 5 - Create S3 bucket + Load data + Enable static website feature + configuring public access block, public objects 20/09/2025 20/09/2025 AWS Study Group 6 - Test Website + Block all public access + Configure Amazon CloudFront + Test Amazon Cloudfront 21/09/2025 21/09/2025 AWS Study Group Week 10 Achievements: Gained a solid understanding of AWS fundamentals and the main service groups including Compute, Storage, Networking, and Database. Successfully created and configured an AWS Free Tier account for learning and practice purposes. Became familiar with the AWS Management Console and learned how to navigate the interface, search for services, and manage resources. Installed and configured the AWS CLI, including setting up Access Key, Secret Key, and Default Region for command-line operations. Performed essential AWS CLI tasks such as checking configuration details, listing available regions, interacting with EC2 services, and creating or managing key pairs. Developed the ability to manage AWS resources using both the Console and CLI in parallel to improve efficiency and flexibility. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Learn and understand AWS security services and the Shared Responsibility Model. Practice managing IAM, Cognito, Security Hub, Key Management Service, and AWS Organizations. Gain hands-on experience creating and managing VPCs, Security Groups, EC2 instances, Lambda roles, and resource tags. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Share Resonsibility Model - Amazon Identify and access management - Amazon Cognito - AWS ORganiztion, Identify Center , Security Hub - Amazon Key Managemnt Service 17/11/2025 17/11/2025 AWS Study Group 3 - Practice: + Enable Security Hub + Score for each set of criteria + Clean up reoureces 18/11/2025 18/11/2025 AWS Study Group 4 - Create VPC + Create Security Group + Create EC2 instance + Incoming Web-hooks clack + Create Tag for instance + Create Role for Lambda 19/11/2025 19/11/2025 AWS Study Group 5 - Check Result + Function stop instance + Function start instance + Clean up Resources 20/11/2025 20/11/2025 AWS Study Group 6 - Practice: + Create EC2 instance with tag + Managin Tags in AWS Resources, File resources by tag + Using tags with CLI + Clean up Resources 21/11/2025 21/11/2025 AWS Study Group Week 11 Achievements 1. AWS Security \u0026amp; Identity Management\nGained a solid understanding of AWS security services, including IAM, Cognito, Key Management Service, Security Hub, and Organizations. Learned and applied the Shared Responsibility Model for AWS resources. Practiced enabling Security Hub, evaluating security scores, and cleaning up AWS resources. 2. Networking \u0026amp; Compute Setup\nCreated and configured VPCs, Security Groups, EC2 instances, and Lambda roles. Applied tags to AWS resources and learned to manage resources efficiently using tags. 3. Resource Management \u0026amp; Automation\nPracticed starting and stopping EC2 instances using automation functions. Ensured proper resource cleanup. Improved ability to manage AWS resources efficiently using a combination of Console and CLI. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Learn and practice AWS data services including S3, Glue, Athena, DynamoDB, QuickSight, EMR, DataBrew, Kinesis, and Lambda. Understand how to build serverless, event-driven architectures and transform, analyze, and visualize data in AWS. Gain hands-on experience with dataset management, dashboard creation, and cost \u0026amp; resource management. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Create S3 Bucket - Create Sample Data - Create Glue Crawler - Data Check 24/11/2025 24/11/2025 AWS Study Group 3 - S3 store output, Session connect setup - Analysis with Athena,Visualize with QuickSight + Hand-on Labs for Amazon + Explore dynamoDB, dynamoDB console + Back up + Clean up 25/11/2025 25/11/2025 AWS Study Group 4 - Advanced Design Patterns for Amazon DynamoDB - Build a Serverless Event Driven Architecture with DynamoDB + Preparing the database + Building a database + Data in the table + Cost + Tagging And Cost Allocation + Usage + Addition result query + Clean up resource 26/11/2025 26/11/2025 AWS Study Group 5 - Cloushell 2.2 + Console + SDK + Creating a Cloud9 Instance + Download Dataset + Upload Dataset to S3 + Setting up DataBrew + Clean \u0026amp; Tranform data 27/11/2025 27/11/2025 AWS Study Group 6 - Practice: + Tranform Data with AWS Glue , AWS Glue DataBrew + Tranform Data with EMR + Analysis with Athena + Analysis with Kinesis Data Analtytics + Visualize in QuickSight + Serve with Lambda + Build Dashboard + Dashboard Improvemnts + Create Interactive Dashboard 28/11/2025 28/11/2025 AWS Study Group Week 12 Achievements: 1. AWS Data Storage \u0026amp; Management\nCreated and managed S3 buckets with sample data. Built Glue Crawlers and prepared datasets for analysis. Explored DynamoDB, designed database patterns, and implemented serverless event-driven architecture. 2. Data Transformation \u0026amp; Analysis\nUsed AWS Glue, DataBrew, and EMR to transform datasets. Analyzed data with Athena and Kinesis Data Analytics. Practiced uploading, backing up, and cleaning datasets. 3. Visualization \u0026amp; Serverless Integration\nBuilt dashboards and visualizations using QuickSight. Integrated Lambda for data serving and interactivity. Learned to manage costs, tags, and resource allocation efficiently. Improved ability to combine AWS Console and CLI for parallel management of resources. "
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://Danh01092004.github.io/AWS-workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]